"""InVEST Coastal Vulnerability"""
import time
import os
import math
import logging
import pickle

import numpy
from osgeo import gdal
from osgeo import osr
from osgeo import ogr
import pandas
import rtree
import shapely
import shapely.wkb
import shapely.ops
import shapely.speedups
import shapely.errors
import pygeoprocessing
import taskgraph

from . import utils
from . import validation

LOGGER = logging.getLogger(__name__)

_N_FETCH_RAYS = 16


def execute(args):
    """InVEST Coastal Vulnerability Model.

    For points along a coastline, evaluate the relative exposure of points
    to coastal hazards based on up to eight biophysical hazard indices.
    Also quantify the role of habitats in reducing the hazard. Optionally
    summarize the population density in proximity to each shore point.

    Parameters:
        args['workspace_dir'] (string): (required) a path to the directory that
            will write output and other temporary files during calculation.
        args['results_suffix'] (string): (optional) appended to any output
            filename.
        args['aoi_vector_path'] (string): (required) path to a polygon vector
            that is projected in a coordinate system with units of meters.
            The polygon should intersect the landmass and the shelf contour line.
        model_resolution (string): (required) distance in meters. Points
            are spaced at intervals of this distance along the coastline.
        args['landmass_vector_path'] (string): (required) path to a polygon
            vector representing landmasses in the region of interest.
        args['wwiii_vector_path'] (string): (required) path to a point vector
            containing wind and wave information across the region of interest.
        max_fetch_distance (string): (required) maximum distance
            in meters to extend rays from shore points. Points with rays equal
            to this distance will accumulate ocean-driven wave exposure along
            those rays and local-wind-driven wave exposure along the shorter rays.
        args['shelf_contour_vector_path'] (string): (required) path to a
            polyline vector delineating edges of the continental shelf
            or other bathymetry contour.
        args['dem_path'] (string): (required) path to a raster representing the
            elevation on land in the region of interest.
        dem_averaging_radius (int or float): (required) a value >= 0.  The radius
            in meters around each shore point in which to compute the average
            elevation.
        args['habitat_table_path'] (string): (rqeuired) path to a CSV file with the
            following four fields:
                'id': unique string to represent each habitat
                'path': absolute or relative path to a polygon vector
                'rank': integer from 1 to 5 representing the relative protection
                    offered by this habitat
                'protection distance (m)': integer or float used as a
                    search radius around each shore point.
        args['geomorphology_vector_path'] (string): (optional) path to a
            polyline vector that has a field called "RANK" with values from
            1 to 5 in the attribute table.
        args['geomorphology_fill_value'] (int): (optional) a value from 1 to 5
            that will be used as a geomorphology rank for any points not
            proximate to the geomorphology_vector_path.
        args['population_rater_path'] (string): (optional) path a raster with
            values of total population per pixel.
        population_radius (int or float): (optional) a value >= 0.
            The radius in meters around each shore point in which to compute
            the population density.
        args['n_workers'] (int): (optional) The number of worker processes to
            use for processing this model.  If omitted, computation will take
            place in the current process.

    Returns:
        None

    """
    LOGGER.info('Validating arguments')
    invalid_parameters = validate(args)
    if invalid_parameters:
        raise ValueError("Invalid parameters passed: %s" % invalid_parameters)
    _validate_habitat_table_header(args['habitat_table_path'])

    output_dir = os.path.join(args['workspace_dir'])
    intermediate_dir = os.path.join(
        args['workspace_dir'], 'intermediate')
    habitat_dir = os.path.join(
        intermediate_dir, 'habitats')
    shore_dir = os.path.join(
        intermediate_dir, 'shore_points')
    relief_dir = os.path.join(
        intermediate_dir, 'relief')
    geomorph_dir = os.path.join(
        intermediate_dir, 'geomorphology')
    wind_wave_dir = os.path.join(
        intermediate_dir, 'wind_wave')
    surge_dir = os.path.join(
        intermediate_dir, 'surge')
    population_dir = os.path.join(
        intermediate_dir, 'population')
    slr_dir = os.path.join(
        intermediate_dir, 'sealevelrise')

    utils.make_directories(
        [output_dir, intermediate_dir, habitat_dir, shore_dir, relief_dir,
         geomorph_dir, wind_wave_dir, surge_dir, population_dir, slr_dir])
    file_suffix = utils.make_suffix_string(args, 'results_suffix')

    work_token_dir = os.path.join(intermediate_dir, '_taskgraph_working_dir')
    try:
        n_workers = int(args['n_workers'])
    except (KeyError, ValueError, TypeError):
        # KeyError when n_workers is not present in args
        # ValueError when n_workers is an empty string.
        # TypeError when n_workers is None.
        n_workers = -1  # Single process mode.
    task_graph = taskgraph.TaskGraph(work_token_dir, n_workers)

    model_resolution = float(args['model_resolution'])
    max_fetch_distance = float(args['max_fetch_distance'])

    aoi_vector_info = pygeoprocessing.get_vector_info(
        args['aoi_vector_path'])
    aoi_srs_wkt = aoi_vector_info['projection']
    aoi_bounding_box = aoi_vector_info['bounding_box']
    # add the max_fetch_distance to the bounding box so we can use
    # the clipped landmass in the ray casting routine.
    fetch_buffer = max_fetch_distance + model_resolution
    aoi_bounding_box[0] -= fetch_buffer
    aoi_bounding_box[1] -= fetch_buffer
    aoi_bounding_box[2] += fetch_buffer
    aoi_bounding_box[3] += fetch_buffer

    clipped_landmass_path = os.path.join(
        shore_dir, 'clipped_projected_landmass%s.gpkg' % file_suffix)
    tmp_clipped_path = os.path.join(
        shore_dir, 'tmp_clipped_landmass%s.gpkg' % file_suffix)
    clip_landmass_to_aoi_task = task_graph.add_task(
        func=clip_and_project_vector,
        args=(args['landmass_vector_path'], aoi_bounding_box,
              aoi_srs_wkt, tmp_clipped_path, clipped_landmass_path),
        target_path_list=[clipped_landmass_path, tmp_clipped_path],
        hash_algorithm='md5',
        copy_duplicate_artifact=True,
        dependent_task_list=[],
        task_name='clip landmass to aoi')

    target_polygon_pickle_path = os.path.join(
        shore_dir, 'landmass_polygon%s.pickle' % file_suffix)
    target_lines_pickle_path = os.path.join(
        shore_dir, 'landmass_line_index%s.pickle' % file_suffix)
    target_rtree_path = os.path.join(
        shore_dir, 'landmass_line_rtree%s.dat' % file_suffix)
    prep_landmass_geom_task = task_graph.add_task(
        func=prepare_landmass_line_index,
        args=(clipped_landmass_path, target_polygon_pickle_path,
              target_lines_pickle_path, target_rtree_path),
        target_path_list=[target_polygon_pickle_path,
                          target_lines_pickle_path,
                          target_rtree_path],
        hash_algorithm='md5',
        copy_duplicate_artifact=True,
        dependent_task_list=[clip_landmass_to_aoi_task],
        task_name='index landmass geometry')

    shore_point_vector_path = os.path.join(
        shore_dir, 'shore_points%s.gpkg' % file_suffix)
    create_shore_points_task = task_graph.add_task(
        func=interpolate_shore_points,
        args=(args['aoi_vector_path'], target_lines_pickle_path,
              model_resolution, shore_point_vector_path),
        target_path_list=[shore_point_vector_path],
        hash_algorithm='md5',
        copy_duplicate_artifact=True,
        dependent_task_list=[
            clip_landmass_to_aoi_task, prep_landmass_geom_task],
        task_name='create shore points')

    exposure_variables_task_list = []
    exposure_variables_path_list = []  # list of 3-tuples like:
    # ('pickle path' (str), bin values (bool), 'R_name' (str))
    # bin values (bool):
        # True when a variable contains values that need binning
            # by percentile to convert to 1-5 ranks.
        # False when variable is already on the 1-5 rank scale.

    target_wwiii_point_vector_path = os.path.join(
        wind_wave_dir, 'wwiii_shore_points%s.gpkg' % file_suffix)
    interpolate_wwiii_task = task_graph.add_task(
        func=interpolate_wwiii_to_shore,
        args=(shore_point_vector_path, args['wwiii_vector_path'],
              target_wwiii_point_vector_path),
        target_path_list=[target_wwiii_point_vector_path],
        hash_algorithm='md5',
        copy_duplicate_artifact=True,
        dependent_task_list=[create_shore_points_task],
        task_name='interpolate wwiii to shore points')
    exposure_variables_task_list.append(interpolate_wwiii_task)

    fetch_point_vector_path = os.path.join(
        wind_wave_dir, 'fetch_points%s.gpkg' % file_suffix)
    target_wind_exposure_pickle_path = os.path.join(
        wind_wave_dir, 'wind%s.pickle' % file_suffix)
    exposure_variables_path_list.append(
        (target_wind_exposure_pickle_path, True, 'R_wind'))
    wind_exposure_task = task_graph.add_task(
        func=calculate_wind_exposure,
        args=(target_wwiii_point_vector_path,
              target_polygon_pickle_path,
              target_rtree_path,
              target_lines_pickle_path,
              wind_wave_dir, file_suffix,
              max_fetch_distance,
              fetch_point_vector_path,
              target_wind_exposure_pickle_path),
        target_path_list=[fetch_point_vector_path,
                          target_wind_exposure_pickle_path],
        hash_algorithm='md5',
        copy_duplicate_artifact=True,
        dependent_task_list=[interpolate_wwiii_task],
        task_name='calculate wind exposure')
    exposure_variables_task_list.append(wind_exposure_task)

    target_wave_exposure_path = os.path.join(
        wind_wave_dir, 'wave%s.pickle' % file_suffix)
    exposure_variables_path_list.append(
        (target_wave_exposure_path, True, 'R_wave'))
    exposure_variables_task_list.append(task_graph.add_task(
        func=calculate_wave_exposure,
        args=(fetch_point_vector_path, max_fetch_distance,
              target_wave_exposure_path),
        target_path_list=[target_wave_exposure_path],
        dependent_task_list=[wind_exposure_task],
        task_name='calculate wave exposure'))

    target_surge_exposure_path = os.path.join(
        surge_dir, 'surge%s.pickle' % file_suffix)
    exposure_variables_path_list.append(
        (target_surge_exposure_path, True, 'R_surge'))
    exposure_variables_task_list.append(task_graph.add_task(
        func=calculate_surge_exposure,
        args=(shore_point_vector_path, args['shelf_contour_vector_path'],
              args['aoi_vector_path'], target_surge_exposure_path),
        target_path_list=[target_surge_exposure_path],
        hash_algorithm='md5',
        copy_duplicate_artifact=True,
        dependent_task_list=[create_shore_points_task],
        task_name='calculate surge exposure'))

    relief_point_pickle_path = os.path.join(
        relief_dir, 'relief%s.pickle' % file_suffix)
    exposure_variables_path_list.append(
        (relief_point_pickle_path, True, 'R_relief'))
    exposure_variables_task_list.append(task_graph.add_task(
        func=calculate_relief_exposure,
        args=(shore_point_vector_path, args['dem_path'],
              float(args['dem_averaging_radius']), model_resolution,
              relief_dir, file_suffix, relief_point_pickle_path),
        target_path_list=[relief_point_pickle_path],
        hash_algorithm='md5',
        copy_duplicate_artifact=True,
        dependent_task_list=[create_shore_points_task],
        task_name='calculate relief exposure'))

    # Joining this task instead of passing it to this scheduler function.
    # Tasks added by the scheduler are dependent on shore_points_task
    create_shore_points_task.join()
    hab_tasks_list, hab_targets_list = _schedule_habitat_tasks(
        shore_point_vector_path, args['habitat_table_path'],
        habitat_dir, file_suffix, task_graph)

    target_habitat_protection_path = os.path.join(
        habitat_dir, 'habitat_protection%s.csv' % file_suffix)
    exposure_variables_task_list.append(task_graph.add_task(
        func=calculate_habitat_rank,
        args=(hab_targets_list,
              target_habitat_protection_path),
        target_path_list=[target_habitat_protection_path],
        hash_algorithm='md5',
        copy_duplicate_artifact=True,
        dependent_task_list=hab_tasks_list,
        task_name='calculate habitat protection'))

    if 'geomorphology_vector_path' in args and args['geomorphology_vector_path'] != '':
        projected_geomorphology_vector_path = os.path.join(
            geomorph_dir, 'geomorphology_projected%s.shp' % file_suffix)
        target_srs_wkt = pygeoprocessing.get_vector_info(
            args['aoi_vector_path'])['projection']
        project_geomorph_task = task_graph.add_task(
            func=pygeoprocessing.reproject_vector,
            args=(args['geomorphology_vector_path'], target_srs_wkt,
                  projected_geomorphology_vector_path),
            target_path_list=[projected_geomorphology_vector_path],
            hash_algorithm='md5',
            copy_duplicate_artifact=True,
            task_name='project geomorphology input')

        target_geomorphology_pickle_path = os.path.join(
            geomorph_dir, 'geomorph%s.pickle' % file_suffix)
        exposure_variables_path_list.append(
            (target_geomorphology_pickle_path, False, 'R_geomorph'))
        exposure_variables_task_list.append(task_graph.add_task(
            func=calculate_geomorphology_exposure,
            args=(projected_geomorphology_vector_path,
                  args['geomorphology_fill_value'],
                  shore_point_vector_path, model_resolution,
                  target_geomorphology_pickle_path),
            target_path_list=[
                target_geomorphology_pickle_path],
            hash_algorithm='md5',
            copy_duplicate_artifact=True,
            dependent_task_list=[
                create_shore_points_task, project_geomorph_task],
            task_name='calculate geomorphology exposure'))

    if 'slr_vector_path' in args and args['slr_vector_path'] != '':
        target_slr_pickle_path = os.path.join(
            slr_dir, 'slr%s.pickle' % file_suffix)
        exposure_variables_path_list.append(
            (target_slr_pickle_path, True, 'R_slr'))
        exposure_variables_task_list.append(task_graph.add_task(
            func=interpolate_sealevelrise_points,
            args=(shore_point_vector_path, args['slr_vector_path'],
                  args['slr_field'], target_slr_pickle_path),
            target_path_list=[target_slr_pickle_path],
            hash_algorithm='md5',
            copy_duplicate_artifact=True,
            dependent_task_list=[create_shore_points_task],
            task_name='interpolate sea-level rise values'))

    if 'population_raster_path' in args and args['population_raster_path'] != '':
        target_population_pickle_path = os.path.join(
            population_dir, 'population%s.pickle' % file_suffix)
        exposure_variables_path_list.append(
            (target_population_pickle_path, False, None))
        exposure_variables_task_list.append(task_graph.add_task(
            func=aggregate_population_density,
            args=(shore_point_vector_path, args['population_raster_path'],
                  float(args['population_radius']), model_resolution,
                  population_dir, file_suffix, target_population_pickle_path),
            target_path_list=[target_population_pickle_path],
            hash_algorithm='md5',
            copy_duplicate_artifact=True,
            dependent_task_list=[create_shore_points_task],
            task_name='aggregate population raster'))

    target_exposure_vector_path = os.path.join(
        output_dir, 'coastal_exposure%s.gpkg' % file_suffix)
    target_exposure_csv_path = os.path.splitext(
        target_exposure_vector_path)[0] + '.csv'
    target_intermediate_vector_path = os.path.join(
        intermediate_dir, 'intermediate_exposure%s.gpkg' % file_suffix)
    task_graph.add_task(
        func=calculate_final_risk,
        args=(exposure_variables_path_list, target_habitat_protection_path,
              shore_point_vector_path, target_intermediate_vector_path,
              target_exposure_vector_path,
              target_exposure_csv_path),
        target_path_list=[
            target_exposure_vector_path, target_exposure_csv_path,
            target_intermediate_vector_path],
        hash_algorithm='md5',
        copy_duplicate_artifact=True,
        dependent_task_list=exposure_variables_task_list,
        task_name='calculate final risk')

    task_graph.close()
    task_graph.join()


def aggregate_population_density(
        base_shore_point_vector_path, base_population_raster_path,
        search_radius, model_resolution, workspace_dir, file_suffix,
        target_pickle_path):
    """Get population density within a search radius of points.

    Parameters:
        base_shore_point_vector_path (string): path to point vector
        base_population_raster_path (string): path to raster with population
            values
        search_radius (float): radius in meters around each point to search
            for valid population pixels.
        workspace_dir (string): path to directory for intermediate files
        file_suffix (string): appended to any output filename.
        target_pickle_path (string): path to pickle file storing dict
            keyed by shore point fid.

    Returns:
        None
    """

    LOGGER.info("Aggregating population data")
    # SRS here is inherited from the user's AOI
    base_shore_point_info = pygeoprocessing.get_vector_info(
        base_shore_point_vector_path)
    target_srs_wkt = base_shore_point_info['projection']

    # extend the clipping box to accomodate the search radius,
    # plus a little extra to avoid nodata within radius after warping.
    shore_point_bounding_box = base_shore_point_info['bounding_box']
    shore_point_bounding_box[0] -= search_radius
    shore_point_bounding_box[1] -= search_radius
    shore_point_bounding_box[2] += search_radius
    shore_point_bounding_box[3] += search_radius

    clipped_projected_pop_path = os.path.join(
        workspace_dir, 'clipped_projected_pop%s.tif' % file_suffix)
    clip_and_project_raster(
        base_population_raster_path, shore_point_bounding_box, target_srs_wkt,
        model_resolution, workspace_dir, file_suffix, clipped_projected_pop_path)

    _aggregate_raster_values_in_radius(
        base_shore_point_vector_path, clipped_projected_pop_path,
        search_radius, target_pickle_path, aggregation_mode='density')


def interpolate_shore_points(
        aoi_vector_path, landmass_lines_pickle_path, model_resolution,
        target_vector_path):
    """Create points along a polyline geometry at a specified interval.

    Parameters:
        aoi_vector_path (string): path to polygon vector used to define
            boundaries for adding points.
        landmass_lines_pickle_path (string): path to pickle file
            containing list of shapely linestrings
        model_resolution (float): distance in meters for the point's spacing
        target_vector_path (string): path to .gpkg point vector

    Returns:
        None
    """

    LOGGER.info("creating shore points along edge of landmass")
    # create the spatial reference from the base vector
    aoi_vector_info = pygeoprocessing.get_vector_info(aoi_vector_path)
    aoi_spatial_reference = osr.SpatialReference()
    aoi_spatial_reference.ImportFromWkt(aoi_vector_info['projection'])

    aoi_vector = gdal.OpenEx(aoi_vector_path, gdal.OF_VECTOR)
    aoi_layer = aoi_vector.GetLayer()
    aoi_shapely_list = []
    for feature in aoi_layer:
        geom = feature.GetGeometryRef()
        aoi_shapely_list.append(shapely.wkb.loads(geom.ExportToWkb()))
    aoi_shapely = shapely.ops.cascaded_union(aoi_shapely_list)
    aoi_shapely_prepped = shapely.prepared.prep(aoi_shapely)
    aoi_vector = None
    aoi_layer = None

    with open(landmass_lines_pickle_path, 'rb') as lines_pickle_file:
        shapely_line_list = pickle.load(lines_pickle_file)
    lines_in_aoi_list = []
    for shapely_geom in shapely_line_list:
        # Already clipped landlines to AOI, with max-fetch extent
        # still want to clip lines by actual AOI, for shore point creation
        if aoi_shapely_prepped.intersects(shapely_geom):
            intersected_shapely_geom = aoi_shapely.intersection(shapely_geom)
            if not intersected_shapely_geom.is_empty:
                if intersected_shapely_geom.type == 'MultiLineString':
                    shapely_geom_explode = [
                        shapely.geometry.LineString(x)
                        for x in intersected_shapely_geom]
                    lines_in_aoi_list.extend(shapely_geom_explode)
                if intersected_shapely_geom.type in ['Point', 'MultiPoint']:
                    # a polygon-line intersection could generate a point geom
                    continue
                else:
                    lines_in_aoi_list.append(intersected_shapely_geom)
    unioned_line = shapely.ops.linemerge(lines_in_aoi_list)

    point_list = []
    for line in unioned_line:
        n_pts = int(math.ceil(line.length / model_resolution))
        points_along = [
            line.interpolate((float(i)/n_pts), normalized=True)
            for i in range(1, n_pts)]
        point_list.extend(points_along)

    gpkg_driver = ogr.GetDriverByName("GPKG")
    target_vector = gpkg_driver.CreateDataSource(target_vector_path)
    layer_name = os.path.splitext(
        os.path.basename(target_vector_path))[0].encode('utf-8')
    target_layer = target_vector.CreateLayer(
        layer_name, aoi_spatial_reference, ogr.wkbPoint)

    target_defn = target_layer.GetLayerDefn()
    for point_feature in point_list:
        geometry = ogr.Geometry(ogr.wkbPoint)
        geometry.AddPoint(
            point_feature.coords[0][0], point_feature.coords[0][1])
        feature = ogr.Feature(target_defn)
        feature.SetGeometry(geometry)
        target_layer.CreateFeature(feature)

    target_layer = None
    target_vector = None


def interpolate_sealevelrise_points(
        base_shore_point_vector_path, slr_points_vector_path,
        slr_fieldname, target_pickle_path):

    max_slr_distance = 500000  # meters

    base_srs_wkt = pygeoprocessing.get_vector_info(
        slr_points_vector_path)['projection']
    base_spatial_reference = osr.SpatialReference()
    base_spatial_reference.ImportFromWkt(base_srs_wkt)
    target_srs_wkt = pygeoprocessing.get_vector_info(
        base_shore_point_vector_path)['projection']
    target_spatial_reference = osr.SpatialReference()
    target_spatial_reference.ImportFromWkt(target_srs_wkt)
    transform_slr_to_shore = osr.CoordinateTransformation(
        base_spatial_reference, target_spatial_reference)

    slr_rtree = rtree.index.Index()
    slr_vector = gdal.OpenEx(
        slr_points_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    slr_layer = slr_vector.GetLayer()
    for feature in slr_layer:
        geometry = feature.GetGeometryRef()
        geometry.Transform(transform_slr_to_shore)
        slr_x = geometry.GetX()
        slr_y = geometry.GetY()
        slr_rtree.insert(
            feature.GetFID(), (slr_x, slr_y, slr_x, slr_y))
        geometry = None
        feature = None

    result = {}
    base_vector = gdal.OpenEx(
        base_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    base_layer = base_vector.GetLayer()
    for feature in base_layer:
        shore_fid = feature.GetFID()
        shore_geometry = feature.GetGeometryRef()
        shore_x = shore_geometry.GetX()
        shore_y = shore_geometry.GetY()
        nearest_points = list(slr_rtree.nearest(
            (shore_x, shore_y, shore_x, shore_y), 2))[0:2]

        nearest_slr_values = numpy.empty((2, 1))
        slr_points = numpy.empty((2, 2))
        for fid_index, fid in enumerate(nearest_points):
            slr_feature = slr_layer.GetFeature(fid)
            slr_geometry = slr_feature.GetGeometryRef()
            slr_geometry.Transform(transform_slr_to_shore)
            slr_points[fid_index] = numpy.array(
                [slr_geometry.GetX(), slr_geometry.GetY()])
            try:
                nearest_slr_values[fid_index] = float(
                    slr_feature.GetField(slr_fieldname))
            except KeyError:
                raise KeyError(
                    'fieldname %s not found in %s' % (
                        slr_fieldname, slr_points_vector_path))

        distances = numpy.linalg.norm(
            slr_points - numpy.array(
                (shore_x, shore_y)), axis=1)

        # make sure points are within a valid data distance
        close_enough = distances < max_slr_distance
        if not any(close_enough):
            LOGGER.warning("No sea-level-rise data for shore point")
            # TODO: What should happen here? raise an exception?
            # So far, model is not setup to handle missing data later on
            continue

        slr_value = numpy.average(
            nearest_slr_values[close_enough],
            weights=(max_slr_distance - distances[close_enough]),
            axis=0)
        result[shore_fid] = slr_value[0]

    slr_layer = None
    slr_vector = None
    base_layer = None
    base_vector = None

    with open(target_pickle_path, 'wb') as pickle_file:
        pickle.dump(result, pickle_file)


def interpolate_wwiii_to_shore(
        base_shore_point_vector_path, wwiii_vector_path,
        target_shore_point_vector_path):
    """Calculate wave exposure for each shore point.
    Join tabular data from Wave Watch 3 to shore points by finding the
    nearest WW3 points to each shore point.

    Parameters:
        base_shore_point_vector_path (string): path to point vector
        wwiii_vector_path (string): path to point shapefile representing
            the Wave Watch III data.
        target_shore_point_vector_path (string): path to point vector file
            with interpolated wwiii data.

    Returns:
        None

    """
    # Wave Watch III data does not cover the planet.  Make sure we don't take
    # values from a point across the ocean:
    _max_wwiii_distance = 5.0  # degrees

    LOGGER.info("Building spatial index for Wave Watch III points")
    wwiii_rtree = rtree.index.Index()
    wwiii_vector = gdal.OpenEx(wwiii_vector_path, gdal.OF_VECTOR)
    wwiii_layer = wwiii_vector.GetLayer()
    for wwiii_feature in wwiii_layer:
        wwiii_geometry = wwiii_feature.GetGeometryRef()
        wwiii_x = wwiii_geometry.GetX()
        wwiii_y = wwiii_geometry.GetY()
        wwiii_rtree.insert(
            wwiii_feature.GetFID(), (wwiii_x, wwiii_y, wwiii_x, wwiii_y))

    # Copy shore point geometry and create fields for WWIII values
    # _copy_gpkg_rename_layer(
    _copy_point_vector_to_gpkg(
        base_shore_point_vector_path, target_shore_point_vector_path)
    points_vector = gdal.OpenEx(
        target_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_Update)
    points_layer = points_vector.GetLayer()
    wwiii_defn = wwiii_layer.GetLayerDefn()
    field_names = []
    for field_index in range(wwiii_defn.GetFieldCount()):
        field_defn = wwiii_defn.GetFieldDefn(field_index)
        field_name = field_defn.GetName()
        if field_name in ['I', 'J']:
            continue
        field_names.append(field_name)
        points_layer.CreateField(field_defn)

    wwiii_spatial_reference = osr.SpatialReference()
    wwiii_ref_wkt = pygeoprocessing.get_vector_info(
        wwiii_vector_path)['projection']
    wwiii_spatial_reference.ImportFromWkt(wwiii_ref_wkt)
    # In case a user decided to project this input on their own:
    if bool(wwiii_spatial_reference.IsProjected()):
        _max_wwiii_distance = 350000  # meters

    points_spatial_reference = osr.SpatialReference()
    points_ref_wkt = pygeoprocessing.get_vector_info(
        target_shore_point_vector_path)['projection']
    points_spatial_reference.ImportFromWkt(points_ref_wkt)
    points_to_wwiii_transform = osr.CoordinateTransformation(
        points_spatial_reference, wwiii_spatial_reference)

    LOGGER.info("Interpolating Wave Watch III data to shore points")
    wwiii_field_lookup = {}
    for shore_point_feature in points_layer:
        shore_point_geometry = shore_point_feature.GetGeometryRef()
        # Transform each shore point to match the wwiii SRS
        shore_point_longitude, shore_point_latitude, _ = (
            points_to_wwiii_transform.TransformPoint(
                shore_point_geometry.GetX(), shore_point_geometry.GetY()))
        # From wave watch III points within AOI, get nearest from shore point
        nearest_points = list(wwiii_rtree.nearest(
            (shore_point_longitude, shore_point_latitude,
             shore_point_longitude, shore_point_latitude), 3))[0:3]

        # create placeholders for point geometry and field values
        wwiii_points = numpy.empty((3, 2))
        wwiii_values = numpy.empty((3, len(field_names)))
        for fid_index, fid in enumerate(nearest_points):
            wwiii_feature = wwiii_layer.GetFeature(fid)
            wwiii_geometry = wwiii_feature.GetGeometryRef()
            wwiii_points[fid_index] = numpy.array(
                [wwiii_geometry.GetX(), wwiii_geometry.GetY()])
            try:
                wwiii_values[fid_index] = wwiii_field_lookup[fid]
            except KeyError:
                wwiii_field_lookup[fid] = numpy.array(
                    [float(wwiii_feature.GetField(field_name))
                     for field_name in field_names])
                wwiii_values[fid_index] = wwiii_field_lookup[fid]

        distances = numpy.linalg.norm(
            wwiii_points - numpy.array(
                (shore_point_longitude,
                 shore_point_latitude)), axis=1)

        # make sure points are within a valid data distance
        close_enough = distances < _max_wwiii_distance
        if not any(close_enough):
            LOGGER.warning("No WWIII data for shore point")
            # TODO: What should happen here? raise an exception?
            # So far, model is not setup to handle missing data later on
            continue

        wwiii_values = numpy.average(
            wwiii_values[close_enough],
            weights=(_max_wwiii_distance - distances[close_enough]),
            axis=0)

        for field_name_index, field_name in enumerate(field_names):
            shore_point_feature.SetField(
                field_name, wwiii_values[field_name_index])
        points_layer.SetFeature(shore_point_feature)
        shore_point_feature = None

    points_layer = None
    points_vector = None


def prepare_landmass_line_index(
        landmass_vector_path, target_polygon_pickle_path,
        target_lines_pickle_path, target_rtree_path):
    """Prepare landmass polygon geometry for intersections.

    Saves unioned geometry polygons for fast point-in-polygon checks.
    Converts landmass polygons to lines used in shore point interpolation
    and saves spatial index of lines for fast intersections.

    Parameters:
        landmass_vector_path (string): path to polygon vector
        target_polygon_pickle_path (string): path to pickle
            storing shapely polygon geometry.
        target_lines_pickle_path (string): path to pickle
            storing list of shapely line geometries
        target_rtree_path (string): path to rtree file indexing
            bounds of line geometries.

    Returns:
        None
    """

    LOGGER.info("preparing landmass geometry")
    # Get shapely geometries from landmass
    landmass_polygon_shapely_list = []
    landmass_polygon_vector = gdal.OpenEx(
        landmass_vector_path, gdal.OF_VECTOR | gdal.GA_Update)
    landmass_polygon_layer = landmass_polygon_vector.GetLayer()
    for feature in landmass_polygon_layer:
        geometry = feature.GetGeometryRef()
        shapely_geometry = shapely.wkb.loads(
            geometry.ExportToWkb())
        if shapely_geometry.is_valid:
            # TODO: if geom is invalid we are just skipping it?
            landmass_polygon_shapely_list.append(shapely_geometry)
        geometry = None
        feature = None
    landmass_polygon_layer = None
    landmass_polygon_vector = None
    landmass_shapely = shapely.ops.cascaded_union(
        landmass_polygon_shapely_list)
    landmass_polygon_shapely_list = None

    # store polygon geom for point-in-poly check later in ray-casting
    with open(target_polygon_pickle_path, 'wb') as polygon_pickle_file:
        pickle.dump(landmass_shapely, polygon_pickle_file)

    # Build index of landmass line geometries on disk
    target_rtree_base = os.path.splitext(target_rtree_path)[0]
    if os.path.exists(target_rtree_path):
        for ext in ['.dat', '.idx']:
            os.remove(target_rtree_base + ext)

    polygon_line_rtree = rtree.index.Index(target_rtree_base)
    shapely_line_index = []
    line_id = 0
    LOGGER.info("indexing geometry of landmass")
    for line in geometry_to_lines(landmass_shapely):
        if (line.bounds[0] == line.bounds[2] and
                line.bounds[1] == line.bounds[3]):
            continue
        polygon_line_rtree.insert(line_id, line.bounds)
        line_id += 1
        shapely_line_index.append(line)

    with open(target_lines_pickle_path, 'wb') as lines_pickle_file:
        pickle.dump(shapely_line_index, lines_pickle_file)
    polygon_line_rtree.close()


def calculate_wind_exposure(
        base_shore_point_vector_path, landmass_polygon_pickle_path,
        landmass_line_rtree_path, landmass_lines_pickle_path,
        workspace_dir, file_suffix, max_fetch_distance,
        target_shore_point_vector_path, target_wind_exposure_pickle_path):
    """Calculate wind exposure for each shore point.

    Parameters:
        base_shore_point_vector_path (string): path to a point shapefile
            representing shore points that should be sampled for wind
            exposure.
        landmass_vector_path (string): path to landmass polygon vetor.
        workspace_dir (string): path to a directory that can be created for
            temporary workspace files
        model_resolution (float): smallest feature size to detect in
            meters.
        max_fetch_distance (float): maximum fetch distance for a ray in
            meters.
        target_fetch_point_vector_path (string): path to target point file,
            will be a copy of `base_shore_point_vector_path`'s geometry with
            an 'REI' (relative exposure index) field added.

    Returns:
        None

    """
    LOGGER.info("Calculating wind exposure")
    temp_fetch_rays_path = os.path.join(
        workspace_dir, 'fetch_rays%s.gpkg' % file_suffix)

    # this should still match the user-defined SRS from the AOI:
    base_ref_wkt = pygeoprocessing.get_vector_info(
        base_shore_point_vector_path)['projection']
    base_spatial_reference = osr.SpatialReference()
    base_spatial_reference.ImportFromWkt(base_ref_wkt)

    gpkg_driver = ogr.GetDriverByName('GPKG')

    # Get prepared polygon geom for point-in-poly checks
    with open(landmass_polygon_pickle_path, 'rb') as polygon_pickle_file:
        landmass_shapely = pickle.load(polygon_pickle_file)
    landmass_shapely_prep = shapely.prepared.prep(landmass_shapely)

    with open(landmass_lines_pickle_path, 'rb') as lines_pickle_file:
        shapely_line_index = pickle.load(lines_pickle_file)

    # load an existing rtree from disk
    polygon_line_rtree = rtree.index.Index(
        os.path.splitext(landmass_line_rtree_path)[0])

    # create fetch rays
    temp_fetch_rays_vector = gpkg_driver.CreateDataSource(
        temp_fetch_rays_path)
    layer_name = os.path.splitext(
        os.path.basename(temp_fetch_rays_path))[0].encode('utf-8')
    temp_fetch_rays_layer = (
        temp_fetch_rays_vector.CreateLayer(
            layer_name, base_spatial_reference, ogr.wkbLineString))
    temp_fetch_rays_defn = temp_fetch_rays_layer.GetLayerDefn()
    temp_fetch_rays_layer.CreateField(ogr.FieldDefn(
        'fetch_dist', ogr.OFTReal))

    base_shore_point_vector = ogr.Open(base_shore_point_vector_path)
    gpkg_driver.CopyDataSource(
        base_shore_point_vector, target_shore_point_vector_path)
    base_shore_point_vector = None

    target_shore_point_vector = gdal.OpenEx(
        target_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_Update)
    target_shore_point_layer = target_shore_point_vector.GetLayer()
    target_shore_point_layer.CreateField(
        ogr.FieldDefn('REI', ogr.OFTReal))
    for ray_index in range(_N_FETCH_RAYS):
        compass_degree = int(ray_index * 360 / 16.)
        target_shore_point_layer.CreateField(
            ogr.FieldDefn('fdist_%d' % compass_degree, ogr.OFTReal))

    shore_point_logger = _make_logger_callback(
        "Wind exposure %.2f%% complete.", LOGGER)
    # Iterate over every shore point
    LOGGER.info("Casting rays ")
    result_REI = {}
    for shore_point_feature in target_shore_point_layer:
        shore_point_fid = shore_point_feature.GetFID()
        shore_point_logger(
            float(shore_point_fid) /
            target_shore_point_layer.GetFeatureCount())
        rei_value = 0.0
        # Iterate over every ray direction
        for sample_index in range(_N_FETCH_RAYS):
            compass_degree = int(sample_index * 360 / 16.)
            compass_theta = float(sample_index) / _N_FETCH_RAYS * 360
            rei_pct = shore_point_feature.GetField(
                'REI_PCT%d' % int(compass_theta))
            rei_v = shore_point_feature.GetField(
                'REI_V%d' % int(compass_theta))
            cartesian_theta = -(compass_theta - 90)

            # Determine the direction the ray will point
            delta_x = math.cos(cartesian_theta * math.pi / 180)
            delta_y = math.sin(cartesian_theta * math.pi / 180)

            # Start a ray offset from the shore point
            # so that rays start outside of the landmass.
            # Shore points are interpolated onto the coastline,
            # but floating point error results in points being just
            # barely inside/outside the landmass.
            offset = 1  # 1 meter should be plenty
            shore_point_geometry = shore_point_feature.GetGeometryRef()
            point_a_x = (
                shore_point_geometry.GetX() + delta_x * offset)
            point_a_y = (
                shore_point_geometry.GetY() + delta_y * offset)
            point_b_x = point_a_x + delta_x * (
                max_fetch_distance)
            point_b_y = point_a_y + delta_y * (
                max_fetch_distance)
            shore_point_geometry = None

            # build ray geometry so we can intersect it later
            ray_geometry = ogr.Geometry(ogr.wkbLineString)
            ray_geometry.AddPoint(point_a_x, point_a_y)
            ray_geometry.AddPoint(point_b_x, point_b_y)

            # keep a shapely version of the ray so we can do fast intersection
            # with it and the entire landmass
            ray_point_origin_shapely = shapely.geometry.Point(
                point_a_x, point_a_y)

            ray_length = 0.0
            if not landmass_shapely_prep.intersects(
                    ray_point_origin_shapely):
                # the origin is in ocean

                # This algorithm searches for intersections, if one is found
                # the ray updates and a smaller intersection set is determined
                # by experimentation I've found this is significant, but not
                # an order of magnitude, faster than looping through all
                # original possible intersections.  Since this algorithm
                # will be run for a long time, it's worth the additional
                # complexity
                tested_indexes = set()
                while True:
                    intersection = False
                    ray_envelope = ray_geometry.GetEnvelope()
                    for poly_line_index in polygon_line_rtree.intersection(
                            [ray_envelope[i] for i in [0, 2, 1, 3]]):
                        if poly_line_index in tested_indexes:
                            continue
                        tested_indexes.add(poly_line_index)
                        line_shapely = (
                            shapely_line_index[poly_line_index])
                        line_segment = ogr.CreateGeometryFromWkb(line_shapely.wkb)
                        if ray_geometry.Intersects(line_segment):
                            # if the ray intersects the poly line, test if
                            # the intersection is closer than any known
                            # intersection so far
                            intersection_point = ray_geometry.Intersection(
                                line_segment)
                            # offset the dist with model_resolution
                            # update the endpoint of the ray
                            ray_geometry = ogr.Geometry(ogr.wkbLineString)
                            ray_geometry.AddPoint(point_a_x, point_a_y)
                            ray_geometry.AddPoint(
                                intersection_point.GetX(),
                                intersection_point.GetY())
                            intersection = True
                            break
                    if not intersection:
                        break
                # when we get here `min_point` and `ray_length` are the
                # minimum intersection points for the ray and the landmass
                ray_feature = ogr.Feature(temp_fetch_rays_defn)
                ray_length = ray_geometry.Length()
                ray_feature.SetField('fetch_dist', ray_length)
                ray_feature.SetGeometry(ray_geometry)
                temp_fetch_rays_layer.CreateFeature(ray_feature)
            shore_point_feature.SetField(
                'fdist_%d' % compass_degree, ray_length)
            ray_feature = None
            ray_geometry = None
            rei_value += ray_length * rei_pct * rei_v
        shore_point_feature.SetField('REI', rei_value)
        target_shore_point_layer.SetFeature(shore_point_feature)
        result_REI[shore_point_fid] = rei_value

    target_shore_point_layer.SyncToDisk()
    target_shore_point_layer = None
    target_shore_point_vector = None
    temp_fetch_rays_layer.SyncToDisk()
    temp_fetch_rays_layer = None
    temp_fetch_rays_vector = None

    with open(target_wind_exposure_pickle_path, 'wb') as pickle_file:
        pickle.dump(result_REI, pickle_file)


def calculate_wave_exposure(
        base_fetch_point_vector_path, max_fetch_distance,
        target_wave_exposure_pickle_path):
    """Calculate wave exposure values at each shore point.

    Parameters:
        base_fetch_point_vector_path (string): path to a point shapefile that
            contains 16 'WavP_[direction]' fields, 'WavPPCT[direction]'
            fields, 'fdist_[direction]' fields, a single H, and a single T
            field.
        max_fetch_distance (float): max fetch distance before a wind fetch ray
            is terminated.
        target_wave_exposure_pickle_path (string): path to pickle file storing
            dict keyed by shore point fid.

    Returns:
        None

    """
    LOGGER.info("Calculating wave exposure")
    result_Ew = {}
    base_fetch_point_vector = gdal.OpenEx(
        base_fetch_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    base_fetch_point_layer = base_fetch_point_vector.GetLayer()
    for base_fetch_point_feature in base_fetch_point_layer:
        point_fid = base_fetch_point_feature.GetFID()
        e_local_wedge = (
            0.5 *
            float(base_fetch_point_feature.GetField('H_10PCT'))**2 *
            float(base_fetch_point_feature.GetField('H_10PCT'))) / float(
                _N_FETCH_RAYS)
        e_ocean = 0.0
        e_local = 0.0
        for sample_index in range(_N_FETCH_RAYS):
            compass_degree = int(sample_index * 360 / 16.)
            fdist = base_fetch_point_feature.GetField(
                'fdist_%d' % compass_degree)
            if numpy.isclose(fdist, max_fetch_distance):
                e_ocean += (
                    base_fetch_point_feature.GetField(
                        'WavP_%d' % compass_degree) *
                    base_fetch_point_feature.GetField(
                        'WavPPCT%d' % compass_degree))
            elif fdist > 0.0:
                e_local += e_local_wedge
        result_Ew[point_fid] = max(e_ocean, e_local)

    with open(target_wave_exposure_pickle_path, 'wb') as pickle_file:
        pickle.dump(result_Ew, pickle_file)


def calculate_surge_exposure(
        base_shore_point_vector_path, shelf_contour_path, aoi_vector_path,
        target_surge_pickle_path):
    """Calculate surge potential as distance to nearest point on a contour.

    Parameters:
        base_shore_point_vector_path (string):  path to a point shapefile to
            for relief point analysis.
        shelf_contour_path (string): path to a polyline vector.
        aoi_vector_path (string): path to a polygon vector.
        target_surge_pickle_path (string): path to pickle file storing dict
            keyed by shore point fid.

    Returns:
        None.

    """
    LOGGER.info("Calculating surge potential")
    aoi_vector_info = pygeoprocessing.get_vector_info(
        aoi_vector_path)
    target_srs_wkt = aoi_vector_info['projection']
    target_spatial_reference = osr.SpatialReference()
    target_spatial_reference.ImportFromWkt(target_srs_wkt)
    aoi_bounding_box = aoi_vector_info['bounding_box']

    base_srs_wkt = pygeoprocessing.get_vector_info(
        shelf_contour_path)['projection']
    base_spatial_reference = osr.SpatialReference()
    base_spatial_reference.ImportFromWkt(base_srs_wkt)
    transform = osr.CoordinateTransformation(
        base_spatial_reference, target_spatial_reference)

    base_srs_clipping_box = pygeoprocessing.transform_bounding_box(
        aoi_bounding_box, target_srs_wkt, base_srs_wkt, edge_samples=11)
    base_srs_clipping_geom = ogr.CreateGeometryFromWkt(
        shapely.geometry.box(*base_srs_clipping_box).wkt)

    base_vector = gdal.OpenEx(
        shelf_contour_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    base_layer = base_vector.GetLayer()

    shapely_geometry_list = []
    for feature in base_layer:
        geometry = feature.GetGeometryRef()
        clipped_geometry = geometry.Intersection(base_srs_clipping_geom)
        if not clipped_geometry.IsEmpty():
            err_code = clipped_geometry.Transform(transform)
            if err_code != 0:
                LOGGER.warning(
                    "Could not transform feature from %s to spatial reference"
                    "system of %s" % (shelf_contour_path, aoi_vector_path))
                continue
            shapely_geom = shapely.wkb.loads(clipped_geometry.ExportToWkb())
            if shapely_geom.type == 'MultiLineString':
                shapely_geom_explode = [
                    shapely.geometry.LineString(x) for x in shapely_geom]
                shapely_geometry_list.extend(shapely_geom_explode)
            else:
                shapely_geometry_list.append(shapely_geom)

    if not shapely_geometry_list:
        raise ValueError(
            "The AOI polygon (%s) and the shelf contour line (%s)"
            "do not intersect" % (aoi_vector_path, shelf_contour_path))

    shelf_shapely_union = shapely.ops.linemerge(
        shapely_geometry_list)

    shore_point_vector = gdal.OpenEx(
        base_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    shore_point_layer = shore_point_vector.GetLayer()
    result = {}
    for point_feature in shore_point_layer:
        point_fid = point_feature.GetFID()
        point_geometry = point_feature.GetGeometryRef()
        point_shapely = shapely.wkb.loads(point_geometry.ExportToWkb())
        result[point_fid] = point_shapely.distance(shelf_shapely_union)

    with open(target_surge_pickle_path, 'wb') as pickle_file:
        pickle.dump(result, pickle_file)


def calculate_relief_exposure(
        base_shore_point_vector_path, base_dem_path, dem_averaging_radius,
        model_resolution, workspace_dir, file_suffix, target_relief_pickle_path):
    """Calculate average of DEM pixels within a radius of shore points.

    Parameters:
        base_shore_point_vector_path (string):  path to a shore point vector.
        base_dem_path (string): path to a DEM raster.
        dem_averaging_radius (float): distance in meters
        workspace_dir (string): path to a directory for intermediate files
        file_suffix (string): to be appended to output filenames
        target_relief_pickle_path (string): path to pickle file storing dict
            keyed by shore point fid.

    Returns:
        None.

    """
    LOGGER.info("Calculating relief exposure")
    base_shore_point_info = pygeoprocessing.get_vector_info(
        base_shore_point_vector_path)
    target_srs_wkt = base_shore_point_info['projection']

    shore_point_bounding_box = base_shore_point_info['bounding_box']
    shore_point_bounding_box[0] -= dem_averaging_radius
    shore_point_bounding_box[1] -= dem_averaging_radius
    shore_point_bounding_box[2] += dem_averaging_radius
    shore_point_bounding_box[3] += dem_averaging_radius

    clipped_projected_dem_path = os.path.join(
        workspace_dir, 'clipped_projected_dem%s.tif' % file_suffix)
    clip_and_project_raster(
        base_dem_path, shore_point_bounding_box, target_srs_wkt,
        model_resolution, workspace_dir, file_suffix,
        clipped_projected_dem_path)

    nodata = pygeoprocessing.get_raster_info(
        clipped_projected_dem_path)['nodata'][0]

    def zero_negative_values(depth_array):
        """Convert negative values to zero."""
        result_array = numpy.empty(depth_array.shape, dtype=numpy.int16)
        if nodata is not None:
            valid_mask = depth_array != nodata
            result_array[:] = nodata
            result_array[valid_mask] = 0
        else:
            result_array[:] = 0
        result_array[depth_array > 0] = depth_array[depth_array > 0]
        return result_array

    positive_dem_path = os.path.join(
        workspace_dir, 'positive_dem%s.tif' % file_suffix)

    pygeoprocessing.raster_calculator(
        [(clipped_projected_dem_path, 1)], zero_negative_values,
        positive_dem_path, gdal.GDT_Int16, nodata)

    _aggregate_raster_values_in_radius(
        base_shore_point_vector_path, positive_dem_path, dem_averaging_radius,
        target_relief_pickle_path)


def _schedule_habitat_tasks(
        base_shore_point_vector_path, habitat_table_path,
        working_dir, file_suffix, task_graph):

    habitat_dataframe = (pandas.read_csv(
        habitat_table_path, header=0,
        dtype={'id': str, 'path': str, 'rank': numpy.int16,
               'protection distance (m)': numpy.int16})
        .rename(columns={'protection distance (m)': 'distance'}))

    habitat_task_list = []
    habitat_pickles_list = []
    for habitat_row in habitat_dataframe.itertuples():
        base_habitat_path = _sanitize_path(
            habitat_table_path, habitat_row.path)
        target_habitat_pickle_path = os.path.join(
            working_dir, '%s%s.pickle' %
            (habitat_row.id, file_suffix))
        habitat_pickles_list.append(target_habitat_pickle_path)
        habitat_task_list.append(task_graph.add_task(
            func=search_for_habitat,
            args=(base_shore_point_vector_path,
                  habitat_row.distance,
                  habitat_row.rank,
                  habitat_row.id,
                  base_habitat_path,
                  target_habitat_pickle_path),
            target_path_list=[target_habitat_pickle_path],
            task_name='searching for %s' % habitat_row.id))

    return habitat_task_list, habitat_pickles_list


def search_for_habitat(
        base_shore_point_vector_path, search_radius, habitat_rank, 
        habitat_id, habitat_vector_path, target_habitat_pickle_path):

    base_shore_point_info = pygeoprocessing.get_vector_info(
        base_shore_point_vector_path)

    shore_point_bounding_box = base_shore_point_info['bounding_box']
    shore_point_bounding_box[0] -= search_radius
    shore_point_bounding_box[1] -= search_radius
    shore_point_bounding_box[2] += search_radius
    shore_point_bounding_box[3] += search_radius

    base_srs_wkt = pygeoprocessing.get_vector_info(
        habitat_vector_path)['projection']
    base_spatial_reference = osr.SpatialReference()
    base_spatial_reference.ImportFromWkt(base_srs_wkt)
    target_srs_wkt = base_shore_point_info['projection']
    target_spatial_reference = osr.SpatialReference()
    target_spatial_reference.ImportFromWkt(target_srs_wkt)
    transform = osr.CoordinateTransformation(
        base_spatial_reference, target_spatial_reference)

    base_srs_clipping_box = pygeoprocessing.transform_bounding_box(
        shore_point_bounding_box, target_srs_wkt, base_srs_wkt, edge_samples=11)
    base_srs_clipping_geom = ogr.CreateGeometryFromWkt(
        shapely.geometry.box(*base_srs_clipping_box).wkt)

    habitat_vector = gdal.OpenEx(
        habitat_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    habitat_layer = habitat_vector.GetLayer()

    # Union all the habitat geometry so that we can easily
    # measure distance from each point to nearest habitat.
    shapely_geometry_list = []
    for feature in habitat_layer:
        geometry = feature.GetGeometryRef()
        clipped_geometry = geometry.Intersection(base_srs_clipping_geom)
        if not clipped_geometry.IsEmpty():
            err_code = clipped_geometry.Transform(transform)
            if err_code != 0:
                LOGGER.warning(
                    "Could not transform feature from %s to"
                    " spatial reference system of AOI"
                    % (habitat_vector_path))
                continue
            shapely_geom = shapely.wkb.loads(clipped_geometry.ExportToWkb())
            if shapely_geom.type == 'MultiPolygon':
                shapely_geom_explode = [
                    shapely.geometry.Polygon(x) for x in shapely_geom]
                shapely_geometry_list.extend(shapely_geom_explode)
            else:
                shapely_geometry_list.append(shapely_geom)
    unioned_habitat_shapely = shapely.ops.cascaded_union(shapely_geometry_list)
    habitat_layer = None
    habitat_vector = None

    base_shore_point_vector = gdal.OpenEx(
        base_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    base_shore_point_layer = base_shore_point_vector.GetLayer()

    result = {habitat_id: {}}
    for point_feature in base_shore_point_layer:
        point_habitat_rank = 5  # represents no habitat protection
        fid = point_feature.GetFID()

        if unioned_habitat_shapely.is_empty:
            result[habitat_id][fid] = point_habitat_rank
            continue

        point_feature_geometry = point_feature.GetGeometryRef()
        point_shapely = shapely.wkb.loads(
            point_feature_geometry.ExportToWkb())

        point_distance_to_feature = point_shapely.distance(
            unioned_habitat_shapely)
        if point_distance_to_feature <= search_radius:
            point_habitat_rank = habitat_rank
        result[habitat_id][fid] = point_habitat_rank

    with open(target_habitat_pickle_path, 'wb') as pickle_file:
        pickle.dump(result, pickle_file)


def calculate_habitat_rank(habitat_pickle_list, target_habitat_protection_path):
    """Combine dicts of habitat ranks into a dataframe and calcuate Rhab."""
    habitat_id_list = []
    dataframe = pandas.DataFrame()
    for hab_pickle in habitat_pickle_list:
        with open(hab_pickle, 'rb') as file:
            data = pickle.load(file)
        habitat_id = data.keys()[0]
        dataframe[habitat_id] = data[habitat_id].values()
        habitat_id_list.append(habitat_id)
    dataframe['fid'] = data[habitat_id].keys()

    def _calc_Rhab(row):
        """Equation 4 from User's Guide."""
        sum_sq_rank = 0.0
        min_rank = 5
        for r in row:
            if r < min_rank:
                min_rank = r
            sum_sq_rank += (5 - r)**2

        if sum_sq_rank > 0:
            r_hab_val = max(
                1.0, 4.8 - 0.5 * (
                    (1.5 * (5 - min_rank))**2 + sum_sq_rank -
                    (5 - min_rank)**2)**0.5)
        else:
            r_hab_val = 5.0
        return r_hab_val

    dataframe['R_hab'] = (
        dataframe.drop(columns='fid')
        .apply(axis=1, func=_calc_Rhab))

    # reorder the columns so 'fid' is always first and 'R_hab' last
    (dataframe[['fid'] + habitat_id_list + ['R_hab']]
        .to_csv(target_habitat_protection_path, index=False))


def calculate_geomorphology_exposure(
        geomorphology_vector_path, geomorphology_fill_value,
        base_shore_point_vector_path, model_resolution, target_pickle_path):
    """
    Buffer point by half of model model_resolution
    union features of each geomorph rank (or index them in rtree)
    for each rank, check if buffer intersects
    """
    LOGGER.info("Assigning geomorphology rank")
    vector = gdal.OpenEx(geomorphology_vector_path)
    acceptable_ranks = (1, 2, 3, 4, 5)
    geomorph_shapely_lookup = {}
    for rank in acceptable_ranks:
        LOGGER.info("unioning geomorphology rank %d" % rank)
        layer = vector.GetLayer()
        filter_string = ('RANK = %d' % rank)
        layer.SetAttributeFilter(str(filter_string))
        rank_geom_list = []
        for feature in layer:
            geometry = feature.GetGeometryRef()
            shapely_geom = shapely.wkb.loads(geometry.ExportToWkb())
            if not shapely_geom.is_empty:
                if shapely_geom.type == 'MultiLineString':
                    shapely_geom_explode = [shapely.geometry.LineString(x) for x in shapely_geom]
                    rank_geom_list.extend(shapely_geom_explode)
                else:
                    rank_geom_list.append(shapely_geom)
        geomorph_shapely_lookup[rank] = shapely.ops.linemerge(rank_geom_list)
        layer = None
    vector = None

    points_vector = gdal.OpenEx(
        base_shore_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    points_layer = points_vector.GetLayer()
    results = {}
    LOGGER.info("searching for geomorph in point buffers")
    for point_feature in points_layer:
        fid = point_feature.GetFID()
        point_geom = point_feature.GetGeometryRef()
        poly_geom = point_geom.Buffer(model_resolution / 2.0)
        poly_shapely = shapely.wkb.loads(poly_geom.ExportToWkb())
        geomorph_presence_bool = [
            poly_shapely.intersects(geomorph_shapely_lookup[r]) for r in acceptable_ranks]
        if any(geomorph_presence_bool):
            r_list = [r for (r, i) in zip(acceptable_ranks, geomorph_presence_bool) if i]
            mean_geomorph = sum(r_list) / float(len(r_list))
        else:
            mean_geomorph = geomorphology_fill_value
        results[fid] = mean_geomorph

    with open(target_pickle_path, 'wb') as pickle_file:
        pickle.dump(results, pickle_file)


def calculate_final_risk(
        risk_id_path_list, habitat_protection_path, base_point_vector_path,
        target_intermediate_vector_path, target_point_vector_path,
        target_point_csv_path):
    """Calculate the final Rt and Rtnohabs for cur, ssp1, 3, and 5.

    Parameters:
        risk_id_list (list): a list of tuples of the form

    Returns:
        None.

    """
    LOGGER.info("Assembling exposure variables")

    final_risk = 'exposure'
    habitat_role = 'habitat_role'
    risk_no_habitat = 'exposure_no_habitats'

    _copy_point_vector_to_gpkg(
        base_point_vector_path, target_point_vector_path)
    target_point_vector = gdal.OpenEx(
        target_point_vector_path, gdal.OF_VECTOR | gdal.GA_Update)
    point_layer = target_point_vector.GetLayer()

    _copy_point_vector_to_gpkg(
        base_point_vector_path, target_intermediate_vector_path)
    target_intermediate_vector = gdal.OpenEx(
        target_intermediate_vector_path, gdal.OF_VECTOR | gdal.GA_Update)
    intermediate_layer = target_intermediate_vector.GetLayer()

    # Intermediate habitat ranks are in a CSV
    habitat_dataframe = pandas.read_csv(habitat_protection_path)
    R_hab_name = 'R_hab'
    habitat_field = ogr.FieldDefn(str(R_hab_name), ogr.OFTReal)
    habitat_field.SetWidth(24)
    habitat_field.SetPrecision(11)
    point_layer.CreateField(habitat_field)

    # Copy precalculated R_hab to the output vector
    for feature_id, rank in zip(
            habitat_dataframe['fid'], habitat_dataframe[R_hab_name]):
        feature = point_layer.GetFeature(int(feature_id))
        feature.SetField(str(R_hab_name), float(rank))
        point_layer.SetFeature(feature)

    # Intermediate risk ranks except habitat are in pickles
    # Bin values to ranks if needed, and copy ranks to output vector
    risk_id_list_no_hab = []
    final_values_dict = {}
    intermediate_values_dict = {}
    for pickle_path, to_bins, R_name in risk_id_path_list:
        if R_name:
            risk_id_list_no_hab.append(R_name)
            fieldname = R_name
        else:
            fieldname = os.path.basename(os.path.splitext(pickle_path)[0])
        exposure_field = ogr.FieldDefn(str(fieldname), ogr.OFTReal)
        exposure_field.SetWidth(24)
        exposure_field.SetPrecision(11)
        point_layer.CreateField(exposure_field)

        if to_bins:
            with open(pickle_path, 'rb') as file:
                raw_values_dict = pickle.load(file)
            intermediate_layer.CreateField(exposure_field)
            invert_values = False
            if R_name == 'R_relief':
                invert_values = True
            values_dict = _bin_values_to_percentiles(
                raw_values_dict, invert_values)
            final_values_dict[fieldname] = values_dict
            intermediate_values_dict[fieldname] = raw_values_dict
        else:
            with open(pickle_path, 'rb') as file:
                final_values_dict[fieldname] = pickle.load(file)

    for feature in point_layer:
        fid = feature.GetFID()
        for fieldname in final_values_dict:
            feature.SetField(
                str(fieldname), float(final_values_dict[fieldname][fid]))
        point_layer.SetFeature(feature)
    point_layer = None
    target_point_vector.FlushCache()
    target_point_vector = None

    # Copy pre-binned intermediate values to intermediate output vector
    for feature in intermediate_layer:
        fid = feature.GetFID()
        for fieldname in intermediate_values_dict:
            feature.SetField(
                str(fieldname), float(intermediate_values_dict[fieldname][fid]))
        intermediate_layer.SetFeature(feature)
    intermediate_layer = None
    target_intermediate_vector.FlushCache()
    target_intermediate_vector = None

    # Now that all the data is compiled, calculate the final exposure
    target_point_vector = gdal.OpenEx(
        target_point_vector_path, gdal.OF_VECTOR | gdal.GA_Update)
    target_result_point_layer = target_point_vector.GetLayer()
    target_result_point_layer.ResetReading()
    target_result_point_layer.CreateField(
        ogr.FieldDefn(final_risk, ogr.OFTReal))
    target_result_point_layer.CreateField(
        ogr.FieldDefn(habitat_role, ogr.OFTReal))
    target_result_point_layer.CreateField(
        ogr.FieldDefn(risk_no_habitat, ogr.OFTReal))

    LOGGER.info("calculating final risk scores")
    risk_id_list = risk_id_list_no_hab + [R_hab_name]
    target_result_point_layer.StartTransaction()
    for target_feature in target_result_point_layer:
        r_list = numpy.array([
            target_feature.GetField(risk_id)
            for risk_id in risk_id_list])
        r_tot = numpy.prod(r_list)**(1./len(r_list))
        target_feature.SetField(final_risk, float(r_tot))
        # User's Guide: calculate "coastal_exposure_no_habitats"
        # by replacing R_hab with 5:
        r_list_nohab = numpy.array([
            target_feature.GetField(risk_id)
            for risk_id in risk_id_list_no_hab] + [5.0])
        r_tot_no_hab = numpy.prod(r_list_nohab)**(1./len(r_list_nohab))
        target_feature.SetField(
            risk_no_habitat, float(r_tot_no_hab))
        target_feature.SetField(
            habitat_role, abs(float(r_tot) - float(r_tot_no_hab)))
        target_result_point_layer.SetFeature(target_feature)
    target_result_point_layer.CommitTransaction()
    target_point_vector.FlushCache()

    csv_driver = gdal.GetDriverByName('CSV')
    vector_copy = csv_driver.CreateCopy(
        target_point_csv_path, target_point_vector)
    vector_copy = None  # forces flush to disk

    target_result_point_layer = None
    target_point_vector = None


def _bin_values_to_percentiles(
        base_values_dict, invert_values=False):

    base_values = base_values_dict.values()
    if invert_values:
        base_values = numpy.multiply(base_values, -1.0)
    target_risk_array = numpy.searchsorted(
                numpy.percentile(
                    base_values, [20, 40, 60, 80, 100]),
                base_values) + 1
    fids = base_values_dict.keys()
    target_values_dict = {}
    for fid, rank in zip(fids, target_risk_array):
        target_values_dict[fid] = rank
    return target_values_dict


def clip_and_project_raster(
        base_raster_path, clipping_box, target_srs_wkt, model_resolution,
        working_dir, file_suffix, target_raster_path):
    """Clip a raster to a box in the raster's native SRS, then reproject."""
    base_srs_wkt = pygeoprocessing.get_raster_info(
        base_raster_path)['projection']

    # 'base' and 'target' srs are with respect to the base and target raster,
    # so first the clipping box needs to go from 'target' to 'base' srs
    base_srs_clipping_box = pygeoprocessing.transform_bounding_box(
        clipping_box, target_srs_wkt, base_srs_wkt, edge_samples=11)

    clipped_raster_path = os.path.join(
        working_dir,
        os.path.basename(
            os.path.splitext(
                base_raster_path)[0]) + '_clipped%s.tif' % file_suffix)

    base_pixel_size = pygeoprocessing.get_raster_info(
        base_raster_path)['pixel_size']
    target_pixel_size = (model_resolution, model_resolution * -1)

    # Clip in the raster's native srs
    pygeoprocessing.warp_raster(
        base_raster_path, base_pixel_size, clipped_raster_path,
        'bilinear', target_bb=base_srs_clipping_box)

    # Warp to the target SRS
    pygeoprocessing.warp_raster(
        clipped_raster_path, target_pixel_size, target_raster_path,
        'bilinear', target_sr_wkt=target_srs_wkt)


def clip_and_project_vector(
        base_vector_path, clipping_box, target_srs_wkt,
        tmp_vector_path, target_vector_path):
    """Clip a vector to a box in the vector's native SRS, then reproject.


    Parameters:
    base_vector_path (string): path to polygon or multipolygon type vector
    clipping_box (list): sequence of floats that are coordinates in the 
        target_srs [minx, miny, maxx, maxy]
    target_srs_wkt (string): well-known-text formatted spatial reference system
    tmp_vector_path (string): path to clipped but unprojected .gpkg vector
    target_vector_path (string): path to clipped and projected .gpkg vector
    """
    for path in [tmp_vector_path, target_vector_path]:
        if os.path.exists(path):
            os.remove(path)

    base_srs_wkt = pygeoprocessing.get_vector_info(
        base_vector_path)['projection']
    base_spatial_reference = osr.SpatialReference()
    base_spatial_reference.ImportFromWkt(base_srs_wkt)

    # 'base' and 'target' srs are with respect to the base and target vector,
    # so first the clipping box needs to go from 'target' to 'base'
    base_srs_clipping_box = pygeoprocessing.transform_bounding_box(
        clipping_box, target_srs_wkt, base_srs_wkt, edge_samples=11)
    base_srs_clipping_box_shapely = shapely.geometry.box(
        *base_srs_clipping_box)

    gpkg_driver = ogr.GetDriverByName("GPKG")
    clipped_vector = gpkg_driver.CreateDataSource(
        tmp_vector_path)
    layer_name = os.path.splitext(
        os.path.basename(tmp_vector_path))[0].encode('utf-8')
    clipped_layer = (
        clipped_vector.CreateLayer(  # intersection could yield MultiPolygon
            layer_name, base_spatial_reference, ogr.wkbMultiPolygon))
    clipped_defn = clipped_layer.GetLayerDefn()

    base_vector = gdal.OpenEx(
        base_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    base_layer = base_vector.GetLayer()
    for feature in base_layer:
        try:
            feature_shapely = shapely.wkb.loads(
                feature.GetGeometryRef().ExportToWkb())
        except shapely.errors.WKBReadingError:
            feature = None
            LOGGER.warning(
                "Couldn't load a feature in %s", base_vector_path)
            continue
        if base_srs_clipping_box_shapely.intersects(feature_shapely):
            intersection_shapely = base_srs_clipping_box_shapely.intersection(
                feature_shapely)
            clipped_geometry = ogr.CreateGeometryFromWkt(
                intersection_shapely.wkt)
            if clipped_geometry.GetGeometryType() == ogr.wkbPolygon:
                clipped_geometry = ogr.ForceToMultiPolygon(clipped_geometry)
            clipped_feature = ogr.Feature(clipped_defn)
            clipped_feature.SetGeometry(clipped_geometry)
            clipped_layer.CreateFeature(clipped_feature)
            clipped_feature = None

    clipped_layer.SyncToDisk()
    clipped_layer = None
    clipped_vector = None
    base_vector = None
    base_layer = None

    pygeoprocessing.reproject_vector(
        tmp_vector_path, target_srs_wkt, target_vector_path,
        driver_name='GPKG')


def _aggregate_raster_values_in_radius(
        base_point_vector_path, base_raster_path, sample_distance,
        target_pickle_path, aggregation_mode='mean'):
    """Aggregate raster values in radius around a point.

    Do the radial search by constructing a rectangular kernel mask
    that approximates a circle.

    E.g. kernel_mask for a raster with pixel_size = (1,1)
    and sample_distance = 3:

    array([[False, False, False,  True, False, False, False],
           [False,  True,  True,  True,  True,  True, False],
           [False,  True,  True,  True,  True,  True, False],
           [ True,  True,  True,  True,  True,  True,  True],
           [False,  True,  True,  True,  True,  True, False],
           [False,  True,  True,  True,  True,  True, False],
           [False, False, False,  True, False, False, False]])

    Parameters:
        base_point_vector_path (string): point vector with projected
            coordinates in units matching sample_disatnce.
        base_raster_path (string): raster file with square pixels and
            projection matching base_point_vector_path.
        sample_distance (float): radius around each point to search
            for valid pixels.
        aggregation_mode (string): 'mean' or 'density'. Calculate the mean
            of valid pixels in search window, or if 'density', divide the mean
            by the area of valid pixels. 'density' assumes units of meters
            for sample_distance and raster geotransform.
        target_pickle_path (string): path to pickle file storing dict
            keyed by point fid.

    Returns:
        None

    """
    if aggregation_mode not in ['mean', 'density']:
        raise ValueError('aggregation mode must be either "mean" or "density"')

    raster = gdal.Open(base_raster_path)
    band = raster.GetRasterBand(1)
    n_rows = band.YSize
    n_cols = band.XSize
    geotransform = raster.GetGeoTransform()
    nodata = band.GetNoDataValue()

    # we can assume square pixels at this point because
    # we already warped input raster and defined square pixels
    pixel_dist = int(abs(
        sample_distance / (geotransform[1])))

    # kernel dimensions will be (2*sample_distance - 1) so that
    # point feature is always inside the center pixel of the kernel.
    # create rectangular kernel and a mask so it looks circular.
    X, Y = numpy.ogrid[:(1 + pixel_dist), :(1 + pixel_dist)]
    lr_quadrant = numpy.hypot(X, Y)
    ll_quadrant = numpy.flip(lr_quadrant[:, 1:], axis=1)
    bottom_half = numpy.concatenate((ll_quadrant, lr_quadrant), axis=1)
    top_half = numpy.flip(bottom_half[1:, :], axis=0)
    kernel_index_distances = numpy.concatenate((top_half, bottom_half), axis=0)

    kernel_mask = numpy.where(
        kernel_index_distances > pixel_dist, False, True)
    LOGGER.warning(kernel_mask)

    result = {}
    vector = gdal.OpenEx(
        base_point_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    layer = vector.GetLayer()
    for point_feature in layer:
        warn_msg = 0
        win_xsize = kernel_mask.shape[0]
        win_ysize = kernel_mask.shape[1]
        temp_kernel_mask = kernel_mask[:]
        LOGGER.warning('starting window x and y sizes')
        LOGGER.warning(win_xsize)
        LOGGER.warning(win_ysize)

        point_fid = point_feature.GetFID()
        point_geometry = point_feature.GetGeometryRef()
        point_x = point_geometry.GetX()
        point_y = point_geometry.GetY()
        point_geometry = None

        # kernel origin (upper-left) in pixel-space
        pixel_x = int(
            (point_x - geotransform[0]) /
            geotransform[1]) - pixel_dist
        pixel_y = int(
            (point_y - geotransform[3]) /
            geotransform[5]) - pixel_dist
        LOGGER.warning(point_y)
        LOGGER.warning(pixel_dist)
        LOGGER.warning(geotransform)

        # if kernel origin is outside bounds of the raster
        if pixel_x < 0:
            warn_msg = 1
            win_xsize += pixel_x
            pixel_x = 0
            # trim mask columns by indexing backward from the right
            temp_kernel_mask = temp_kernel_mask[:, (-1 * win_xsize):]
            LOGGER.warning(temp_kernel_mask)
        if pixel_y < 0:
            LOGGER.warning(pixel_y)
            LOGGER.warning(win_ysize)
            warn_msg = 1
            LOGGER.warning('reassigning')
            win_ysize += pixel_y
            pixel_y = 0
            LOGGER.warning(pixel_y)
            LOGGER.warning(win_ysize)
            # trim mask rows by indexing backward from the bottom
            temp_kernel_mask = temp_kernel_mask[(-1 * win_ysize):, :]
            LOGGER.warning(temp_kernel_mask)

        # if kernel extent is outside bounds of the raster
        if pixel_x + win_xsize > n_cols:
            warn_msg = 1
            win_xsize -= pixel_x + win_xsize - n_cols
            # trim mask columns by indexing forward from left
            temp_kernel_mask = temp_kernel_mask[:, :win_xsize]
            LOGGER.warning(temp_kernel_mask)
        if pixel_y + win_ysize > n_rows:
            warn_msg = 1
            win_ysize -= pixel_y + win_ysize - n_rows
            # trim mask rows by indexing forward from top
            temp_kernel_mask = temp_kernel_mask[:win_ysize, :]
            LOGGER.warning(temp_kernel_mask)
        if warn_msg:
            LOGGER.warning(
                'search radius around point (%.6f, %.6f) '
                'extends beyond bounds of raster %s'
                % (point_x, point_y, base_raster_path))
        if win_xsize <= 0 or win_ysize <= 0:
            pixel_value = 0
        else:
            try:
                array = band.ReadAsArray(
                    xoff=pixel_x, yoff=pixel_y, win_xsize=win_xsize,
                    win_ysize=win_ysize)
                LOGGER.warning(array)
                LOGGER.warning(temp_kernel_mask)
                if nodata is not None:
                    mask = (array != nodata) & temp_kernel_mask
                else:
                    mask = kernel_mask
                if numpy.count_nonzero(mask) > 0:
                    pixel_value = numpy.mean(array[mask])
                else:
                    pixel_value = 0  # TODO: a nodata value would be better
            except Exception:
                LOGGER.error(
                    'band size %d %d', band.XSize,
                    band.YSize)
                raise

        # calculate pixel area in sq km
        if aggregation_mode == 'density':
            pixel_area_km = abs(geotransform[1] * geotransform[5]) / 1e6
            pixel_value /= pixel_area_km

        result[point_fid] = pixel_value
    with open(target_pickle_path, 'wb') as pickle_file:
        pickle.dump(result, pickle_file)


def geometry_to_lines(geometry):
    """Convert a geometry object to a list of lines."""
    if geometry.type == 'Polygon':
        return polygon_to_lines(geometry)
    elif geometry.type == 'MultiPolygon':
        line_list = []
        for geom in geometry.geoms:
            line_list.extend(geometry_to_lines(geom))
        return line_list
    else:
        return []


def polygon_to_lines(geometry):
    """Return a list of shapely lines given higher order shapely geometry."""
    line_list = []
    last_point = geometry.exterior.coords[0]
    for point in geometry.exterior.coords[1::]:
        if point == last_point:
            continue
        line_list.append(shapely.geometry.LineString([last_point, point]))
        last_point = point
    line_list.append(shapely.geometry.LineString([
        last_point, geometry.exterior.coords[0]]))
    for interior in geometry.interiors:
        last_point = interior.coords[0]
        for point in interior.coords[1::]:
            if point == last_point:
                continue
            line_list.append(shapely.geometry.LineString([last_point, point]))
            last_point = point
        line_list.append(shapely.geometry.LineString([
            last_point, interior.coords[0]]))
    return line_list


def _copy_point_vector_to_gpkg(base_vector_path, target_gpkg_path):
    gpkg_driver = ogr.GetDriverByName("GPKG")
    aoi_srs = osr.SpatialReference()
    aoi_srs.ImportFromWkt(
        pygeoprocessing.get_vector_info(base_vector_path)['projection'])

    target_vector = gpkg_driver.CreateDataSource(
        target_gpkg_path)
    layer_name = os.path.splitext(
        os.path.basename(target_gpkg_path))[0].encode('utf-8')
    target_layer = target_vector.CreateLayer(
        layer_name, aoi_srs, ogr.wkbPoint)

    target_layer_defn = target_layer.GetLayerDefn()

    base_vector = gdal.OpenEx(base_vector_path, gdal.OF_VECTOR | gdal.GA_ReadOnly)
    base_layer = base_vector.GetLayer()
    for feature in base_layer:
        target_geometry = ogr.Geometry(ogr.wkbPoint)
        base_geometry = feature.GetGeometryRef()
        target_geometry.AddPoint(base_geometry.GetX(), base_geometry.GetY())
        target_feature = ogr.Feature(target_layer_defn)
        target_feature.SetGeometry(target_geometry)
        target_layer.CreateFeature(target_feature)
        feature = None
        target_feature = None

    target_layer_defn = None
    target_layer = None
    target_vector = None
    base_layer = None
    base_vector = None


def _sanitize_path(base_path, raw_path):
    """Return `raw_path` if absolute, or make absolute relative to `base_path`."""
    if os.path.isabs(raw_path):
        return raw_path
    return os.path.join(os.path.dirname(base_path), raw_path)


def _make_logger_callback(message, logger):
    """Build a timed logger callback that prints `message` replaced.

    Parameters:
        message (string): a string that expects a %f replacement variable for
            `proportion_complete`.

    Returns:
        Function with signature:
            logger_callback(proportion_complete, psz_message, p_progress_arg)

    """
    def logger_callback(proportion_complete):
        """Argument names come from the GDAL API for callbacks."""
        try:
            current_time = time.time()
            if ((current_time - logger_callback.last_time) > 5.0 or
                    (proportion_complete == 1.0 and
                     logger_callback.total_time >= 5.0)):
                LOGGER.info(message, proportion_complete * 100)
                logger_callback.last_time = current_time
                logger_callback.total_time += current_time
        except AttributeError:
            logger_callback.last_time = time.time()
            logger_callback.total_time = 0.0

    return logger_callback


def _validate_habitat_table_header(habitat_table_path):
    dataframe = pandas.read_csv(habitat_table_path)
    header = ('id', 'path', 'rank', 'protection distance (m)')
    missing_names = list(set(header).difference(list(dataframe)))
    if missing_names:
        raise ValueError(
            '%s does not have the following required column names: %s'
            % (habitat_table_path, str(missing_names)))


@validation.invest_validator
def validate(args, limit_to=None):
    """Validate args to ensure they conform to ``execute``'s contract.

    Parameters:
        args (dict): dictionary of key(str)/value pairs where keys and
            values are specified in ``execute`` docstring.
        limit_to (str): (optional) if not None indicates that validation
            should only occur on the ``args[limit_to]`` value. The intent that
            individual key validation could be significantly less expensive
            than validating the entire ``args`` dictionary.

    Returns:
        list of ([invalid key_a, invalid_key_b, ...], 'warning/error message')
            tuples. Where an entry indicates that the invalid keys caused
            the error message in the second part of the tuple. This should
            be an empty list if validation succeeds.

    """
    missing_key_list = []
    no_value_list = []
    validation_error_list = []

    required_keys = [
        'workspace_dir',
        'aoi_vector_path',
        'model_resolution',
        'landmass_vector_path',
        'wwiii_vector_path',
        'max_fetch_distance',
        'shelf_contour_vector_path',
        'dem_path',
        'dem_averaging_radius',
        'habitat_table_path',
        ]

    spatial_files = [
        ('aoi_vector_path', gdal.OF_VECTOR, 'vector'),
        ('landmass_vector_path', gdal.OF_VECTOR, 'vector'),
        ('wwiii_vector_path', gdal.OF_VECTOR, 'vector'),
        ('shelf_contour_vector_path', gdal.OF_VECTOR, 'vector'),
        ('dem_path', gdal.OF_RASTER, 'raster',),
        ]

    polyline_files = [
        'shelf_contour_vector_path'
    ]

    numeric_keys = [
        'max_fetch_distance',
        'dem_averaging_radius',
    ]

    # Some fields are required only when others have values
    if limit_to in [
            None, 'geomorphology_fill_value', 'geomorphology_vector_path']:
        if ('geomorphology_vector_path' in args and
                args['geomorphology_vector_path'] != ''):
            required_keys.append('geomorphology_fill_value')
            spatial_files.append(
                ('geomorphology_vector_path', gdal.OF_VECTOR, 'vector'))
            polyline_files.append('geomorphology_vector_path')
            numeric_keys.append('geomorphology_fill_value')

    if limit_to in [
            None, 'population_raster_path', 'population_radius']:
        if ('population_raster_path' in args and
                args['population_raster_path'] != ''):
            required_keys.append('population_radius')
            spatial_files.append(
                ('population_raster_path', gdal.OF_RASTER, 'raster'))
            numeric_keys.append('population_radius')

    if limit_to in [
            None, 'slr_vector_path', 'slr_field']:
        if ('slr_vector_path' in args and
                args['slr_vector_path'] != ''):
            required_keys.append('slr_field')
            spatial_files.append(
                ('slr_vector_path', gdal.OF_VECTOR, 'vector'))

    for key in required_keys:
        if limit_to in (None, key):
            if key not in args:
                missing_key_list.append(key)
            elif args[key] in ('', None):
                no_value_list.append(key)

    if missing_key_list:
        raise KeyError(*missing_key_list)

    if no_value_list:
        validation_error_list.append(
            (no_value_list, 'parameter has no value'))

    with utils.capture_gdal_logging():
        for key, filetype, filetype_string in spatial_files:
            if limit_to not in (key, None):
                continue

            spatial_file = gdal.OpenEx(args[key], filetype)
            if spatial_file is None:
                validation_error_list.append(
                    ([key], 'Must be a %s' % filetype_string))
            if key in polyline_files:
                layer = spatial_file.GetLayer()
                # TODO: https://gist.github.com/walkermatt/7121427 -- get better ref
                if layer.GetGeomType() not in [2, 5, -2147483646, -2147483643]:
                    validation_error_list.append(
                        ([key], 'Must be a polyline vector'))


    # Verify that the AOI projection is in meters.
    # We don't care about the other spatial inputs, as they'll all be
    # reprojected to the AOI's projection.
    if limit_to in ('aoi_vector_path', None):
        # only do this check if we can open the vector.
        with utils.capture_gdal_logging():
            do_spatial_check = False
            if gdal.OpenEx(args['aoi_vector_path'], gdal.OF_VECTOR) is not None:
                do_spatial_check = True
        if do_spatial_check:
            aoi_srs = osr.SpatialReference()
            aoi_srs.ImportFromWkt(
                pygeoprocessing.get_vector_info(
                    args['aoi_vector_path'])['projection'])
            if (abs(aoi_srs.GetLinearUnits() - 1.0) > 0.5e-7 or
                    not bool(aoi_srs.IsProjected())):
                validation_error_list.append(
                    (['aoi_vector_path'], 'Must be projected in meters'))

    if (limit_to in [None, 'habitat_table_path'] and
            'habitat_table_path' in args):
        if (args['habitat_table_path'] not in [None, ''] and
                not os.path.exists(args['habitat_table_path'])):
            validation_error_list.append(
                (['habitat_table_path'],
                 'not found on disk'))

    for key in numeric_keys:
        if limit_to not in (key, None):
            continue
        try:
            float(args[key])
        except (ValueError, TypeError):
            validation_error_list.append(
                ([key], "Must be a number"))

    return validation_error_list
